{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB4XZmsZjCJZ",
        "outputId": "15d64110-feeb-4bcd-e087-ba53c4e5a88c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Вкупно примероци: 4975\n",
            "Број по дијалект:\n",
            "тетовски (долнополошки): 192\n",
            "скопскоцрногорски: 49\n",
            "кумановски: 771\n",
            "кривопаланечки: 173\n",
            "овчеполски: 7\n",
            "кратовски: 93\n",
            "скопски-велешки: 158\n",
            "кичевско-поречки: 350\n",
            "прилепско-битолски: 626\n",
            "гостиварски (горнополошки): 94\n",
            "галички: 21\n",
            "дебарски: 40\n",
            "вевчанско-радошки: 25\n",
            "струшки: 519\n",
            "охридски: 128\n",
            "горнопреспански: 54\n",
            "долнопреспански: 38\n",
            "тиквешко-мариовски: 248\n",
            "штипско-кочански: 228\n",
            "малешевско-пирински: 397\n",
            "гевгелиско-дојрански: 485\n",
            "струмичко-радовишки: 182\n",
            "дримколско-голобрдски: 97\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/dijalekti/texts_segmented.jsonl\"\n",
        "\n",
        "texts = []\n",
        "dialects = []\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        t = (obj.get(\"text\") or \"\").strip()\n",
        "        d = (obj.get(\"dialect\") or \"\").strip()\n",
        "        if t != \"\" and d != \"\":\n",
        "            texts.append(t)\n",
        "            dialects.append(d)\n",
        "\n",
        "print(\"Вкупно примероци:\", len(texts))\n",
        "\n",
        "cnt = Counter(dialects)\n",
        "print(\"Број по дијалект:\")\n",
        "for d, c in cnt.items():\n",
        "    print(f\"{d}: {c}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lQVe3YdjRDm",
        "outputId": "fdc5b6e0-5624-4fd3-af52-156a31e7666a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Број по дијалект (по нормализација):\n",
            "тетовски (долнополошки): 192\n",
            "скопскоцрногорски: 49\n",
            "кумановски: 771\n",
            "кривопаланечки: 173\n",
            "овчеполски: 7\n",
            "кратовски: 93\n",
            "скопски-велешки: 158\n",
            "кичевско-поречки: 350\n",
            "прилепско-битолски: 626\n",
            "гостиварски (горнополошки): 94\n",
            "галички: 21\n",
            "дебарски: 40\n",
            "вевчанско-радошки: 25\n",
            "струшки: 519\n",
            "охридски: 128\n",
            "горнопреспански: 54\n",
            "долнопреспански: 38\n",
            "тиквешко-мариовски: 248\n",
            "штипско-кочански: 228\n",
            "малешевско-пирински: 397\n",
            "гевгелиско-дојрански: 485\n",
            "струмичко-радовишки: 182\n",
            "дримколско-голобрдски: 97\n",
            "Класи: ['вевчанско-радошки' 'галички' 'гевгелиско-дојрански' 'горнопреспански'\n",
            " 'гостиварски (горнополошки)' 'дебарски' 'долнопреспански'\n",
            " 'дримколско-голобрдски' 'кичевско-поречки' 'кратовски' 'кривопаланечки'\n",
            " 'кумановски' 'малешевско-пирински' 'овчеполски' 'охридски'\n",
            " 'прилепско-битолски' 'скопски-велешки' 'скопскоцрногорски'\n",
            " 'струмичко-радовишки' 'струшки' 'тетовски (долнополошки)'\n",
            " 'тиквешко-мариовски' 'штипско-кочански']\n",
            "Број класи: 23\n",
            "Train: 3980 Val: 696 Test: 299\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "MIN_SAMPLES = 3\n",
        "\n",
        "dialects_norm = []\n",
        "for d in dialects:\n",
        "    dialects_norm.append(\"other\" if cnt[d] < MIN_SAMPLES else d)\n",
        "\n",
        "cnt_norm = Counter(dialects_norm)\n",
        "print(\"Број по дијалект (по нормализација):\")\n",
        "for d, c in cnt_norm.items():\n",
        "    print(f\"{d}: {c}\")\n",
        "\n",
        "le_dialect = LabelEncoder()\n",
        "y_all = le_dialect.fit_transform(dialects_norm)\n",
        "\n",
        "print(\"Класи:\", le_dialect.classes_)\n",
        "print(\"Број класи:\", len(le_dialect.classes_))\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    texts, y_all,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_all\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDlzrTKUjVgf",
        "outputId": "e3f68d63-e50f-4ca7-e7fd-a6f38dccdd26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ќе тренираме на: cuda\n",
            "Број класи: 23\n",
            "Train samples: 3980\n",
            "Val samples: 696\n",
            "Test samples: 299\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"amberoad/bert-multilingual-passage-reranking-msmarco\"\n",
        "max_length = 512\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "lr = 3e-5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Ќе тренираме на:\", device)\n",
        "\n",
        "num_labels = len(le_dialect.classes_)\n",
        "print(\"Број класи:\", num_labels)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class TextDialectDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = list(texts)\n",
        "        self.labels = np.array(labels)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = TextDialectDataset(X_train, y_train, tokenizer, max_length=max_length)\n",
        "val_dataset   = TextDialectDataset(X_val,   y_val,   tokenizer, max_length=max_length)\n",
        "test_dataset  = TextDialectDataset(X_test,  y_test,  tokenizer, max_length=max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Val samples:\", len(val_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmFg6UkFjhv_",
        "outputId": "b0004ccc-19b3-403d-92cd-40b560350569"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at amberoad/bert-multilingual-passage-reranking-msmarco and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([23, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([23]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model device: cuda:0\n",
            "Batch input_ids device (before to(device)): cpu\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "print(\"Model device:\", next(model.parameters()).device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "b = next(iter(train_loader))\n",
        "print(\"Batch input_ids device (before to(device)):\", b[\"input_ids\"].device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86s1u1CujnLZ"
      },
      "outputs": [],
      "source": [
        "def move_batch_to_device(batch, device):\n",
        "    return {\n",
        "        \"input_ids\": batch[\"input_ids\"].to(device, non_blocking=True),\n",
        "        \"attention_mask\": batch[\"attention_mask\"].to(device, non_blocking=True),\n",
        "        \"labels\": batch[\"labels\"].to(device, non_blocking=True),\n",
        "    }\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = move_batch_to_device(batch, device)\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss += loss.item() * batch[\"labels\"].size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=-1).detach().cpu().numpy()\n",
        "            labels = batch[\"labels\"].detach().cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    avg_loss = total_loss / max(1, len(all_labels))\n",
        "    acc = (np.array(all_preds) == np.array(all_labels)).mean() if len(all_labels) else 0.0\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0) if len(all_labels) else 0.0\n",
        "    return avg_loss, acc, f1, np.array(all_labels), np.array(all_preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc_NHHvojwI0",
        "outputId": "37a27879-16dc-4ad1-eaa0-eb393a2e7798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train loss: 2.1390 | Val loss: 1.6468 | Val acc: 0.523 | Val F1(macro): 0.219\n",
            "Epoch 02 | Train loss: 1.2764 | Val loss: 1.1463 | Val acc: 0.655 | Val F1(macro): 0.351\n",
            "Epoch 03 | Train loss: 0.8632 | Val loss: 0.9597 | Val acc: 0.717 | Val F1(macro): 0.424\n",
            "=== TRANSFORMER TEXT – Test ===\n",
            "Test loss: 0.9826 | Test acc: 0.676 | Test F1(macro): 0.375\n",
            "\n",
            "Classification report (тест):\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "         вевчанско-радошки       0.00      0.00      0.00         1\n",
            "                   галички       0.00      0.00      0.00         2\n",
            "      гевгелиско-дојрански       0.83      0.90      0.86        21\n",
            "           горнопреспански       0.00      0.00      0.00         3\n",
            "гостиварски (горнополошки)       1.00      0.20      0.33         5\n",
            "                  дебарски       0.00      0.00      0.00         2\n",
            "           долнопреспански       0.00      0.00      0.00         5\n",
            "     дримколско-голобрдски       0.10      0.29      0.15         7\n",
            "          кичевско-поречки       0.76      0.96      0.85        26\n",
            "                 кратовски       0.00      0.00      0.00         7\n",
            "            кривопаланечки       0.71      1.00      0.83        10\n",
            "                кумановски       0.82      0.97      0.89        37\n",
            "       малешевско-пирински       0.77      0.89      0.83        27\n",
            "                овчеполски       0.00      0.00      0.00         1\n",
            "                  охридски       0.18      0.20      0.19        10\n",
            "        прилепско-битолски       0.85      0.80      0.82        41\n",
            "           скопски-велешки       0.67      0.44      0.53         9\n",
            "         скопскоцрногорски       0.00      0.00      0.00         5\n",
            "       струмичко-радовишки       0.33      0.14      0.20         7\n",
            "                   струшки       0.77      0.82      0.79        33\n",
            "   тетовски (долнополошки)       0.43      0.55      0.48        11\n",
            "        тиквешко-мариовски       0.44      0.29      0.35        14\n",
            "          штипско-кочански       0.50      0.53      0.52        15\n",
            "\n",
            "                  accuracy                           0.68       299\n",
            "                 macro avg       0.40      0.39      0.38       299\n",
            "              weighted avg       0.63      0.68      0.64       299\n",
            "\n"
          ]
        }
      ],
      "source": [
        "best_val_f1 = -1.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = move_batch_to_device(batch, device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_train_loss += loss.item() * batch[\"labels\"].size(0)\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataset)\n",
        "\n",
        "    val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | Train loss: {avg_train_loss:.4f} | \"\n",
        "        f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.3f} | Val F1(macro): {val_f1:.3f}\"\n",
        "    )\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "test_loss, test_acc, test_f1, y_true_test, y_pred_test = evaluate(model, test_loader)\n",
        "print(\"=== TRANSFORMER TEXT – Test ===\")\n",
        "print(f\"Test loss: {test_loss:.4f} | Test acc: {test_acc:.3f} | Test F1(macro): {test_f1:.3f}\")\n",
        "\n",
        "labels_sorted = sorted(np.unique(y_true_test))\n",
        "print(\"\\nClassification report (тест):\")\n",
        "print(classification_report(\n",
        "    y_true_test,\n",
        "    y_pred_test,\n",
        "    labels=labels_sorted,\n",
        "    target_names=[le_dialect.classes_[i] for i in labels_sorted],\n",
        "    zero_division=0\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBFeLsmw4bFh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}